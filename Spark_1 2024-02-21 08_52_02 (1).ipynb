{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94d8911-441f-4d97-a546-7149b3a5c700",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1610051100490008>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext\n",
       "\u001B[0;32m----> 2\u001B[0m sc \u001B[38;5;241m=\u001B[39m SparkContext(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlocal\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfirstApp\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/context.py:202\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n",
       "\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    197\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m    198\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    199\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    200\u001B[0m     )\n",
       "\u001B[0;32m--> 202\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(\n",
       "\u001B[1;32m    205\u001B[0m         master,\n",
       "\u001B[1;32m    206\u001B[0m         appName,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    216\u001B[0m         memory_profiler_cls,\n",
       "\u001B[1;32m    217\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/context.py:488\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n",
       "\u001B[1;32m    485\u001B[0m     callsite \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\u001B[38;5;241m.\u001B[39m_callsite\n",
       "\u001B[1;32m    487\u001B[0m     \u001B[38;5;66;03m# Raise error if there is already a running Spark context\u001B[39;00m\n",
       "\u001B[0;32m--> 488\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n",
       "\u001B[1;32m    489\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run multiple SparkContexts at once; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexisting SparkContext(app=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, master=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    491\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m created by \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m at \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    492\u001B[0m         \u001B[38;5;241m%\u001B[39m (\n",
       "\u001B[1;32m    493\u001B[0m             currentAppName,\n",
       "\u001B[1;32m    494\u001B[0m             currentMaster,\n",
       "\u001B[1;32m    495\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfunction,\n",
       "\u001B[1;32m    496\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfile,\n",
       "\u001B[1;32m    497\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mlinenum,\n",
       "\u001B[1;32m    498\u001B[0m         )\n",
       "\u001B[1;32m    499\u001B[0m     )\n",
       "\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    501\u001B[0m     SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;241m=\u001B[39m instance\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-1610051100490008>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkContext\n\u001B[0;32m----> 2\u001B[0m sc \u001B[38;5;241m=\u001B[39m SparkContext(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlocal\u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfirstApp\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/context.py:202\u001B[0m, in \u001B[0;36mSparkContext.__init__\u001B[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m gateway \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m gateway\u001B[38;5;241m.\u001B[39mgateway_parameters\u001B[38;5;241m.\u001B[39mauth_token \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    197\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    198\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    199\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is not allowed as it is a security risk.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    200\u001B[0m     )\n\u001B[0;32m--> 202\u001B[0m \u001B[43mSparkContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_ensure_initialized\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgateway\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgateway\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_do_init(\n\u001B[1;32m    205\u001B[0m         master,\n\u001B[1;32m    206\u001B[0m         appName,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    216\u001B[0m         memory_profiler_cls,\n\u001B[1;32m    217\u001B[0m     )\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/context.py:488\u001B[0m, in \u001B[0;36mSparkContext._ensure_initialized\u001B[0;34m(cls, instance, gateway, conf)\u001B[0m\n\u001B[1;32m    485\u001B[0m     callsite \u001B[38;5;241m=\u001B[39m SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context\u001B[38;5;241m.\u001B[39m_callsite\n\u001B[1;32m    487\u001B[0m     \u001B[38;5;66;03m# Raise error if there is already a running Spark context\u001B[39;00m\n\u001B[0;32m--> 488\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    489\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run multiple SparkContexts at once; \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    490\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexisting SparkContext(app=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m, master=\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    491\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m created by \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m at \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    492\u001B[0m         \u001B[38;5;241m%\u001B[39m (\n\u001B[1;32m    493\u001B[0m             currentAppName,\n\u001B[1;32m    494\u001B[0m             currentMaster,\n\u001B[1;32m    495\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfunction,\n\u001B[1;32m    496\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mfile,\n\u001B[1;32m    497\u001B[0m             callsite\u001B[38;5;241m.\u001B[39mlinenum,\n\u001B[1;32m    498\u001B[0m         )\n\u001B[1;32m    499\u001B[0m     )\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    501\u001B[0m     SparkContext\u001B[38;5;241m.\u001B[39m_active_spark_context \u001B[38;5;241m=\u001B[39m instance\n\n\u001B[0;31mValueError\u001B[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ",
       "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: Cannot run multiple SparkContexts at once; existing SparkContext(app=Databricks Shell, master=local[8]) created by __init__ at /databricks/python_shell/dbruntime/spark_connection.py:127 ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "sc = SparkContext(\"local\",\"firstApp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9afe092-0def-4f5b-a387-5f5d0129d007",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: '3.3.2'"
     ]
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aea2f227-00cc-420f-aacf-e48316e0b5be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: '3.9'"
     ]
    }
   ],
   "source": [
    "sc.pythonVer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a10d6ef9-6eeb-4e3a-ba14-2bb3d66ec8bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: 'local[8]'"
     ]
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f298b34-7f63-42ed-bf3b-e29739b70375",
     "showTitle": true,
     "title": "Loading data in Pyspark"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: [1, 2, 3, 4, 5, 6]"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1,2,3,4,5,6])\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38bf9e9e-fb77-4120-99b6-dbdb5005d31e",
     "showTitle": true,
     "title": "Loading data in spark using text file"
    }
   },
   "outputs": [],
   "source": [
    "rdd2 = sc.textFile(\"/FileStore/testFile1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22d6b226-afad-45dc-be67-1e6f8af5b1ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: ['1,John,Doe,25',\n '2,Jane,Smith,30',\n '3,Bob,Johnson,22',\n '4,Alice,Williams,28',\n '5,Charlie,Brown,35']"
     ]
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a542672-ca3d-4012-b2e2-30c42c5fd052",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "words = sc.parallelize([\"python\",\"scala\",\"R\",\"Java\",\"PySpark\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd1f5503-f1e7-406a-bd67-b9db112d9de5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fun1(x):\n",
    "    print(f\"Processing: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "754461d2-6802-41eb-a6c9-6b5e2a9067cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3117407782209885>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m for_rdd \u001B[38;5;241m=\u001B[39m words\u001B[38;5;241m.\u001B[39mforeach(fun1)\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(for_rdd\u001B[38;5;241m.\u001B[39mcollect())\n",
       "\n",
       "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'collect'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-3117407782209885>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m for_rdd \u001B[38;5;241m=\u001B[39m words\u001B[38;5;241m.\u001B[39mforeach(fun1)\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(for_rdd\u001B[38;5;241m.\u001B[39mcollect())\n\n\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'collect'",
       "errorSummary": "<span class='ansi-red-fg'>AttributeError</span>: 'NoneType' object has no attribute 'collect'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for_rdd = words.foreach(fun1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05ec1f6e-da5d-4b2e-aba5-b9425cb63c30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: ['python', 'scala', 'R', 'Java', 'PySpark']"
     ]
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5950cf5-d7f7-4c55-8e21-9341ad10330b",
     "showTitle": true,
     "title": "Map Transformation"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: [1, 4, 9, 16, 25, 36]"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1,2,3,4,5,6])\n",
    "rdd2 = rdd1.map(lambda x: x*x)\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92b762c2-b5c7-4c89-ace9-6d3bec6d117a",
     "showTitle": true,
     "title": "Filter Transformation"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5, 6, 7, 8, 9, 12, 23]\n[2, 4, 6, 8, 12]\n[1, 2, 3]\n[6, 9, 12]\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 23]\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 23]\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1,2,3,4,5,6,7,8,9,12,23])\n",
    "rdd2 = rdd1.filter(lambda x: x>2)\n",
    "print(rdd2.collect())\n",
    "\n",
    "rdd3 = rdd1.filter(lambda x: x%2 == 0)\n",
    "print(rdd3.collect())\n",
    "\n",
    "rdd4 = rdd1.filter(lambda x: x<=3)\n",
    "print(rdd4.collect())\n",
    "\n",
    "rdd5 = rdd1.filter(lambda x: x % 3 ==0 and x>5)\n",
    "print(rdd5.collect())\n",
    "\n",
    "\n",
    "rdd6 = rdd1.filter(lambda x: \"Even\" if x% 2 == 0 else \"Odd\")\n",
    "print(rdd6.collect())\n",
    "\n",
    "rdd7 = rdd1.filter(lambda x: \"High\" if x > 5 else \"Low\")\n",
    "print(rdd7.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c399d914-a469-497b-9a04-610439774309",
     "showTitle": true,
     "title": "Flatmap Transformation"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n['Hello', 'world', 'python', 'world']\n[(1, 2), (1, 3), (2, 4), (2, 6), (3, 6), (3, 9), (4, 8), (4, 12), (5, 10), (5, 15), (6, 12), (6, 18)]\n12\n[(1, 2), (1, 3), (2, 4), (2, 6)]\n(1, 2)\n"
     ]
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([[1,2,3],[4,5,6],[7,8,9]])\n",
    "\n",
    "rdd2 = rdd1.flatMap(lambda x:x)\n",
    "print(rdd2.collect())\n",
    "\n",
    "rdd11 = sc.parallelize([\"Hello world\",\"python world\"])\n",
    "rdd3 = rdd11.flatMap(lambda x: x.split(\" \"))\n",
    "print(rdd3.collect())\n",
    "\n",
    "#Example 3: Generating key-value pairs\n",
    "rdd4 = sc.parallelize([1,2,3,4,5,6])\n",
    "rdd5 = rdd4.flatMap(lambda x:[(x,x*2),(x,x*3)])\n",
    "print(rdd5.collect())\n",
    "print(rdd5.count())\n",
    "print(rdd5.take(4))\n",
    "print(rdd5.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "216398f3-2e84-48ef-af02-04199f98c0f1",
     "showTitle": true,
     "title": "Pair RDD create"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'banana'), (2, 'apple'), (3, 'orange'), (4, 'grape')]\n[(1, 2), (2, 4), (3, 6), (4, 8), (5, 10)]\n[(1, 2), (1, 3), (2, 4), (2, 6), (3, 6), (3, 9), (4, 8), (4, 12), (5, 10), (5, 15)]\n[('sam', '24'), ('mary', '23'), ('peter', '34')]\n"
     ]
    }
   ],
   "source": [
    "# Using List of Tuples\n",
    "l1 = [(1,\"banana\"),(2,\"apple\"),(3,\"orange\"),(4,\"grape\")]\n",
    "prdd1 = sc.parallelize(l1)\n",
    "print(prdd1.collect())\n",
    "\n",
    "\n",
    "# From regular RDD [using Map]\n",
    "r_rdd1 = sc.parallelize([1,2,3,4,5])\n",
    "prdd2 = r_rdd1.map(lambda x:(x, x*2))\n",
    "print(prdd2.collect())\n",
    "\n",
    "\n",
    "# From regular RDD [using flatMap]\n",
    "prdd3 = r_rdd1.flatMap(lambda x: [(x,x*2),(x,x*3)])\n",
    "print(prdd3.collect())\n",
    "\n",
    "\n",
    "#From list\n",
    "list1 = [\"sam 24\",\"mary 23\",\"peter 34\"]\n",
    "r_rdd2 = sc.parallelize(list1)\n",
    "prdd4 = r_rdd2.map(lambda x:(x.split(\" \")[0],x.split(\" \")[1]))\n",
    "print(prdd4.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3ad3ec2-9655-4a9a-a043-3a1ca9ba966b",
     "showTitle": true,
     "title": "###Transformation On Pair RDD"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('neya', 36), ('messi', 53), ('rona', 53)]\n"
     ]
    }
   ],
   "source": [
    "# 1 reduceByKey() Transformation: Combines values with the same key\n",
    "\n",
    "rdd1 = sc.parallelize([('messi',23),('rona',34),('neya',36),('messi',30),('rona',19)])\n",
    "prdd5 = rdd1.reduceByKey(lambda x,y:x+y)\n",
    "print(prdd5.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0383f87-2531-4eb6-89a6-db76e281dcac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2 groupByKey() Transformation: groups the values with the same key\n",
    "airports = [('US','jkr'),('UK','LHR'),('FR','CDG'),('UK','SFO'),('US','SFE')]\n",
    "prdd6 = sc.parallelize(airports)\n",
    "gprdd = prdd6.groupByKey().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55c5a9ff-48fd-466c-9ede-e7ff84f78331",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FR ['CDG']\nUK ['LHR', 'SFO']\nUS ['jkr', 'SFE']\n"
     ]
    }
   ],
   "source": [
    "for con,air in gprdd:\n",
    "    print(con,list(air))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a522d04-8d4f-4bcb-b1bc-a205c27b9da5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bird', 2), ('cat', 1), ('dog', 3), ('elephant', 4), ('fish', 5)]\n[('fish', 5), ('elephant', 4), ('dog', 3), ('cat', 1), ('bird', 2)]\n"
     ]
    }
   ],
   "source": [
    "# 3 sortByKey() Transformation:  Return the rdd sorted by the Key\n",
    "prdd7 = sc.parallelize([('dog', 3), ('cat', 1), ('fish', 5), ('bird', 2), ('elephant', 4)])\n",
    "sort_rdd = prdd7.sortByKey()\n",
    "print(sort_rdd.collect())\n",
    "\n",
    "sorted_rdd_desc = prdd7.sortByKey(ascending=False)\n",
    "print(sorted_rdd_desc.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fd049d5-5baf-4fc9-a053-f860f3e1e1d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, ('banana', 'red')), (3, ('orange', 'yellow'))]\n"
     ]
    }
   ],
   "source": [
    "# 4 Join Transformation : Join two pair rdd based on their Key\n",
    "prdd8 = sc.parallelize([(1, 'apple'), (2, 'banana'), (3, 'orange')])\n",
    "prdd9 = sc.parallelize([(2, 'red'), (3, 'yellow'), (4, 'green')])\n",
    "\n",
    "jrdd = prdd8.join(prdd9)\n",
    "print(jrdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f871c378-8dc1-4a10-9e7f-9495dae3a7b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3ebdf41-ab96-4489-8de8-5f775d2a6f37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_1 2024-02-21 08:52:02",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
